
import os
import sys
import time
import pickle
import argparse
import logging
import numpy as np
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch_geometric
from torch_geometric.nn import GCNConv, global_mean_pool
from torch.utils.data import DataLoader, Dataset, RandomSampler
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.metrics import f1_score, accuracy_score, precision_score, hamming_loss, roc_curve, auc
from sklearn.model_selection import KFold
import torch_scatter
from torch_geometric.utils import add_self_loops, degree
from pykeen.models import TransE, TransR
from pykeen.triples import TriplesFactory
from pykeen.pipeline import pipeline
import warnings
warnings.filterwarnings('ignore')   # ingore div 0 error
import matplotlib.pyplot as plt

with open("kg_data\\kg_data.pickle",'rb') as f:
    kg_data = pickle.load(f)
    kg_data_np = np.array(kg_data)
    triples_factory = TriplesFactory.from_labeled_triples(kg_data_np)

def concatenate_entity_tensors(triples_factory, kg_embedding):
    entity_tensors = []
    for i in range(len(triples_factory.entity_to_id.keys())):
        tensor = kg_embedding.entity_representations[0](indices=torch.tensor(triples_factory.entity_to_id[str(i)], dtype=torch.long))
        entity_tensors.append(tensor)
    concatenated_tensor = torch.stack(entity_tensors, dim=0)
    return concatenated_tensor

class GNN_KGmodel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, trained_transR_model):
        super(GNN_KGmodel, self).__init__()
        self.num_classes = num_classes
        self.conv1 = GCNConv(input_dim, hidden_dim) 
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.kg_embedding = trained_transR_model

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = global_mean_pool(x, batch)
        kg_classifier = concatenate_entity_tensors(triples_factory, self.kg_embedding)  # transR

        x = torch.matmul(x, kg_classifier.t())
        return x

# data class
class GraphDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.filenames = os.listdir(data_dir)

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        filename = self.filenames[idx]
        filepath = os.path.join(self.data_dir, filename)
        graph = nx.read_gpickle(filepath)
        try:
            # node embedding
            x = torch.cat([graph.nodes[node]["embedding"] for node in graph.nodes()],dim=0)
            # edge index
            node_list = list(graph.nodes())
            edge_list = list()
            for edge in graph.edges:
                edge_list.append((node_list.index(edge[0]),node_list.index(edge[1])))
            edge_index = torch.tensor(np.array(edge_list).T, dtype=torch.long) 
            # ground truth
            y = np.zeros((1,421))
            for label in graph.graph["label"]:
                y[0][label] = 1
            y = torch.tensor(y, dtype=torch.float)
            data = torch_geometric.data.Data(x=x, edge_index=edge_index, y=y)
            return data
        except Exception as e:  # data error
            # logger.info(f"Error occurred in processing sample {filename}: {e}")
            return self.__getitem__((idx + 1) % len(self.filenames))
    
def collate_fn(data_list):
    batch = torch_geometric.data.Batch.from_data_list(data_list)
    return batch

def delete_folder(folder_path):
    if os.path.exists(folder_path):
        for root, dirs, files in os.walk(folder_path, topdown=False):
            for file in files:
                file_path = os.path.join(root, file)
                os.remove(file_path)
            for dir in dirs:
                dir_path = os.path.join(root, dir)
                os.rmdir(dir_path)
        os.rmdir(folder_path)

def get_logger(name):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename=name)
    logger = logging.getLogger(__name__)
    s_handle = logging.StreamHandler(sys.stdout)
    s_handle.setLevel(logging.INFO)
    s_handle.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(filename)s[:%(lineno)d] - %(message)s"))
    logger.addHandler(s_handle)
    return logger

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Malware2ATT&CK model")
    parser.add_argument("--num_labels", type=int, default=421,help='the number of behaviour')
    parser.add_argument("--graph_path", type=str, default='data\\graph_ebd\\')
    parser.add_argument("--test_path", type=str, default='data\\test_dataset\\')
    parser.add_argument("--batch_size", type=int, default=8)
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger = get_logger(time.strftime('%Y-%m-%d-%H-%M-%S.log', time.localtime()))

    # Set the number of folds
    dataset = GraphDataset(args.graph_path)
    num_folds = 5
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

    test_dataset = GraphDataset(args.test_path)
    test_sampler = RandomSampler(test_dataset)
    test_loader =  DataLoader(test_dataset,  batch_size=args.batch_size, sampler=test_sampler, collate_fn=collate_fn, drop_last=False)

    # load data, train transR
    with open("kg_data\\kg_data.pickle",'rb') as f:
        kg_data = pickle.load(f)
        kg_data_np = np.array(kg_data)
        triples_factory = TriplesFactory.from_labeled_triples(kg_data_np)

    kg_dir = 'models\\transR\\transR_model'
    if not os.path.exists(kg_dir+"\\trained_model.pkl"):
        transR_model = TransR(triples_factory=triples_factory, embedding_dim=1024, scoring_fct_norm=1 )
        kg_result = pipeline(model=transR_model, training=triples_factory, testing=triples_factory, epochs=1000)
        kg_result.save_to_directory(kg_dir)
    trained_transR_model = torch.load(kg_dir+'\\trained_model.pkl')
    
    for fold, (train_index, val_index) in enumerate(kf.split(dataset)):
        train_dataset = torch.utils.data.Subset(dataset, train_index)
        val_dataset = torch.utils.data.Subset(dataset, val_index)
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=collate_fn, drop_last=False)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=collate_fn, drop_last=False)

        logger.info('Load data Done!')
        # define model
        model = GNN_KGmodel(input_dim=1536, hidden_dim=1024, num_classes=args.num_labels, 
                            trained_transR_model=trained_transR_model)
        # set learning rate
        params = list(model.parameters())
        # conv1.bias,conv1.weight,conv2.bias,conv2.weight,entity.weight,relation.weight
        learning_rates = [0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1] 
        optimizer = optim.Adam(params)
        for param_group, lr in zip(optimizer.param_groups, learning_rates):
            param_group['lr'] = lr

        model.to(device)
        for name, param in model.named_parameters():
            logger.info("{}\t{}\t{}".format(name,param.shape,param.requires_grad))
        logger.info('Create model Done!')
        logger.info(model)

        # train
        model.train()
        logger.info('Start train!')
        for epoch in range(10):
            for i, data in enumerate(train_loader):
                try:
                    optimizer.zero_grad()
                    output = model(data.to(device)) 
                    loss = F.binary_cross_entropy_with_logits(output, data.y.to(device))
                    loss.backward()
                    optimizer.step()
                    if i % 100 == 0:
                        logger.info("Epoch {:02d}, Iteration {:04d}, Loss {:.4f}".format(epoch, i, loss.item()))
                        torch.cuda.empty_cache()
                except Exception as e:
                    logger.info("Error occurred in training: {}".format(e))
                    continue
        model.eval()
        with torch.no_grad():
            start_time = time.perf_counter()
            y_true, y_pred = [], []
            for i, data in enumerate(val_loader):
                output = model(data.to(device))
                y_true.append(data.y.cpu().numpy())
                y_pred.append(torch.sigmoid(output).cpu().numpy())
            y_true = np.concatenate(y_true)
            y_pred = np.concatenate(y_pred) > 0.5
            y_pred = np.where(y_pred > 0.5, 1, 0)

            end_time = time.perf_counter()
            print(f"Running time: {end_time - start_time:.2f} second")
            micro_f1 =  f1_score(y_true, y_pred, average='micro')
            macro_f1 =  f1_score(y_true, y_pred, average='macro')
            sample_f1 = f1_score(y_true, y_pred, average='samples')
            accuracy = accuracy_score(y_true, y_pred, normalize=True)   # MR
            ham_loss = hamming_loss(y_true, y_pred)
            micro_precision =  precision_score(y_true, y_pred, average='micro')
            macro_precision =  precision_score(y_true, y_pred, average='macro')
            sample_precision = precision_score(y_true, y_pred, average='samples')

            logger.info("Epoch {:02d}, Iteration {:04d}, Micro   F1:{:.4f}, Acc:{:.4f}, Prc:{:.4f}"
                        .format(epoch, i, micro_f1, accuracy, micro_precision))
            logger.info("Epoch {:02d}, Iteration {:04d}, Macro   F1:{:.4f}, Acc:{:.4f}, Prc:{:.4f}"
                        .format(epoch, i, macro_f1, accuracy, macro_precision))     
            logger.info("Epoch {:02d}, Iteration {:04d}, Samples F1:{:.4f}, Los:{:.4f}, Prc:{:.4f}"
                        .format(epoch, i, sample_f1, ham_loss, sample_precision))  

    # save model
    torch.save(model.state_dict(), "models\\Malware2ATT&CK\\model.bin")
